{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pythonpractice import apps\n",
    "from pythonpractice import REPO_ROOT\n",
    "from pythonpractice import plotting as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "Our friend Roberto owns a cozy little pizzeria in Berlin. Every day at noon, he checks\n",
    "the number of reserved seats and decides how much pizza dough to prepare for dinner. Too\n",
    "much dough, and it goes wasted; too little, and he runs out of pizzas. In either case,\n",
    "he loses money.\n",
    "\n",
    "It’s not always easy to gauge the number of pizzas from the reservations. Many customers\n",
    "don’t reserve a table, or they eat something other than pizza. Roberto knows that there\n",
    "is some kind of link between those numbers, in that more reservations generally mean\n",
    "more pizzas—but other than that, he’s not sure what the exact relation is.\n",
    "\n",
    "We can fit a machine learning model to try and predict this model. We will use\n",
    "Supervised Learning to solve this problem.\n",
    "\n",
    "Here's how the first few lines of the data collected:\n",
    "\n",
    "```text filename=\"pizza.txt\"\n",
    "Reservations  Pizzas\n",
    "13            33\n",
    "2             16\n",
    "14            32\n",
    "23            51\n",
    "```\n",
    "\n",
    "This data is stored in the file `pizza.txt`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the path to the `pizza.txt` file and returns two numpy\n",
    "arrays, one for the `Reservations` column and one for the `Pizzas` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pizza_data(path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # Replace with your code\n",
    "    return np.zeros(()), np.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservations, pizzas = load_pizza_data(str(REPO_ROOT / \"data\" / \"pizza.txt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell and run it to ensure that your function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert reservations.shape == pizzas.shape == (30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.scatter_plot_with_regression_line(\n",
    "#     reservations,\n",
    "#     pizzas,\n",
    "#     title=\"Pizza\",\n",
    "#     x_label=\"Reservations\",\n",
    "#     y_label=\"Pizzas\",\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the above code and run the cell to plot the data. You should see a scatter plot\n",
    "along with a line of best fit. The line of best fit is going to be our estimate of the\n",
    "relationship between the number of reservations and the number of pizzas.\n",
    "\n",
    "We will now build our own linear regression model to predict the number of pizzas\n",
    "given the number of reservations from scratch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in 1D\n",
    "\n",
    "What is the equation of a line in coordinate space? It's $y = mx + b$, where $m$ is the\n",
    "slope of the line and $b$ is the y-intercept. We can use this equation to predict the\n",
    "number of pizzas given the number of reservations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the slope, $m$, the y-intercept, $b$, and the number of\n",
    "reservations, $x$, and returns the predicted number of pizzas, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    reservations: int | np.ndarray, slope: float, intercept: float\n",
    ") -> float | np.ndarray:\n",
    "    # Replace with your code\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're calling the function `predict` because once we have a slope and y-intercept, we\n",
    "can use them to predict the number of pizzas for any number of reservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(predict(10, 0.5, 1), float)\n",
    "assert isinstance(predict(reservations, 0.5, 1), np.ndarray)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guess and Check\n",
    "\n",
    "How do we find the best values for $m$ and $b$? We can try different values and see. We\n",
    "can start by guessing that $m = 1$ and $b = 0$. We can then use our `predict` function\n",
    "to predict the number of pizzas for each number of reservations. We can then compare\n",
    "these predictions to the actual number of pizzas and see how well we did.\n",
    "\n",
    "Btw, $m$ and $b$ are called the **parameters** of the model. We can think of the\n",
    "parameters as knobs that we can turn to change the output of the model. We can use\n",
    "these knobs to make the model better.\n",
    "\n",
    "Say, you have two sets of parameters, $m_1$ and $b_1$, and $m_2$ and $b_2$. How do you\n",
    "know which set of parameters is better? This is where the **loss function** comes in.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function measures how well our model is doing. It takes in the actual number\n",
    "of pizzas and the predicted number of pizzas and returns a number that tells us how\n",
    "bad our model is doing. The lower the number, the better our model is doing.\n",
    "\n",
    "Try to write a function that takes in the actual number of pizzas and the predicted\n",
    "number of pizzas and returns the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_loss_function(y: np.ndarray, y_hat: np.ndarray) -> float:\n",
    "    \"\"\"A loss function that gives you a measure of how well a model performed based on\n",
    "    it's predictions.\n",
    "\n",
    "    Args:\n",
    "        y: The true values\n",
    "        y_hat: The predicted values\n",
    "\n",
    "    Returns:\n",
    "        A float that represents the loss\n",
    "    \"\"\"\n",
    "    # Replace with your code\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What is the loss for $m = 1$, $b = 0$ and the first data point?\n",
    "2. Why does the loss function return a float?\n",
    "3. What are some other loss functions that you can think of?\n",
    "4. What properties should a loss function have?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "The loss function that is typically used for linear regression, and many other machine\n",
    "learning models, is the **mean squared error**. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $n$ is the number of data points, $y_i$ is the actual number of pizzas, and\n",
    "$\\hat{y}_i$ is the predicted number of pizzas.\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. What is the loss for $m = 1$, $b = 0$ and the first data point?\n",
    "2. Why is the mean squared error a good loss function for linear regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the actual number of pizzas and the predicted number of\n",
    "pizzas and returns the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    # Replace with your code\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will plot the data and the line of fit for the chosen values of the\n",
    "parameters. It also contains sliders that you can use to change the values of the\n",
    "parameters and see how the line of fit changes. You can also see the loss for the\n",
    "chosen values of the parameters. Play around with the sliders and see how the line of\n",
    "fit changes. \n",
    "\n",
    "### Questions\n",
    "\n",
    "1. How does the loss function change as the line changes?\n",
    "2. Can you find a combination of parameters that gives you a loss of 0?\n",
    "3. Is it even possible for the linear regression model to get a loss of 0 on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps.visualize_parameter_changes_app(\n",
    "    x=reservations,\n",
    "    y=pizzas,\n",
    "    model=predict,  # type: ignore\n",
    "    parameter_ranges={\n",
    "        \"slope\": apps.ParameterRange(-10, 10),\n",
    "        \"intercept\": apps.ParameterRange(-10, 10),\n",
    "    },\n",
    "    loss_fn=mean_squared_error,\n",
    ").run_server(debug=True, mode=\"external\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, write the code to display the app for your loss function (i.e.\n",
    "`a_loss_function`).\n",
    "\n",
    "### Questions\n",
    "1. How does your loss function change as the line changes?\n",
    "2. Is it as good as the mean squared error? Why or why not?\n",
    "\n",
    "Play around with different loss functions and see how they change as the line changes.\n",
    "All you need to do is define a new function with the same signature as `a_loss_function`\n",
    "and pass it in to the `tweak_model_parameters_app` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in higher dimensions\n",
    "\n",
    "The pizza problem that we're trying to solve above is a linear regression problem in 2\n",
    "dimensions. This is because we have a single input variable, the number of reservations,\n",
    "and a single output variable, the number of pizzas. Such a problem can be visualized\n",
    "in a 2D coordinate space and is called a **Simple Linear Regression** problem.\n",
    "\n",
    "What if we have more than one input variable? For example, what if we also have the\n",
    "number of customers who walk in without a reservation? We can then use the number of\n",
    "reservations and the number of walk-ins to predict the number of pizzas. This is called\n",
    "a **Multiple Linear Regression** problem.\n",
    "\n",
    "The inputs are called **features** and the output are called the **targets** or **labels**.\n",
    "\n",
    "Let's say we have two features, $x_1$ and $x_2$, and one target, $y$. The equation of a\n",
    "plane in 3D space is $z = mx_1 + nx_2 + b$. We can use this equation to predict the\n",
    "number of pizzas given the number of reservations and the number of walk-ins. Again, the\n",
    "parameters of the model are the $m$, $n$, and $b$. Typically, the slopes, $m$ and $n$,\n",
    "are called the **weights** of the model and the y-intercept, $b$, is called the\n",
    "**bias**.\n",
    "\n",
    "Write a function that takes in the weights, $m$ and $n$, the bias, $b$, and the\n",
    "features, $x_1$ and $x_2$, and returns the predicted target, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    x1: float | np.ndarray, x2: float | np.ndarray, m: float, n: float, b: float\n",
    ") -> float | np.ndarray:\n",
    "    # Replace with your code\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the equation for `k` features?\n",
    "\n",
    "Write a function that takes in an arbitrary number of features, corresponding number of\n",
    "weights, and the bias and returns the predicted target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    x: np.ndarray, weights: list[float] | np.ndarray, b: float\n",
    ") -> float | np.ndarray:\n",
    "    # Replace with your code\n",
    "    # Note 1: x is now 2D with shape (n, k) where n is the number of samples and k is\n",
    "    #         the number of features. Check that this is indeed the case.\n",
    "    # Note 2: Check that the number of weights is equal to the number of features\n",
    "    return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix notation\n",
    "\n",
    "We can represent the equation for `k` features as:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^{k} w_i x_i + b\n",
    "$$\n",
    "\n",
    "where $w_i$ is the weight for the $i^{th}$ feature and $x_i$ is the value of the\n",
    "$i^{th}$ feature.\n",
    "\n",
    "We can also represent this equation as:\n",
    "\n",
    "$$\n",
    "y = w^T x + b\n",
    "$$\n",
    "\n",
    "where $w$ is a vector of weights and $x$ is a vector of features. The $w^T$ is the\n",
    "transpose of the vector $w$.\n",
    "\n",
    "Write a function that takes in a vector of weights, a vector of features, and the bias\n",
    "and returns the predicted target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, weights: np.ndarray, b: float) -> np.ndarray:\n",
    "    # Replace with your code\n",
    "    # Note 1: The shape of x is (n, k) where n is the number of samples and k is\n",
    "    #         the number of features\n",
    "    # Note 2: Check that the number of weights is equal to the number of features\n",
    "    return np.zeros(())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical solution\n",
    "\n",
    "Since it's difficult to visualize a problem in more than 3 dimensions, we will go back\n",
    "to simple linear regression to explain further concepts. We will solve the multiple\n",
    "linear regression version too, but we won't be able to visualize it.\n",
    "\n",
    "We can also find the best values for $m$ and $b$ mathematically. We can do this by\n",
    "minimizing the loss function. The best values for $m$ and $b$ are the values that\n",
    "minimize the loss function. We can use calculus to find the values that minimize the\n",
    "loss function.\n",
    "\n",
    "The equation for the best values of $m$ and $b$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial m} \\text{MSE} &= 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} \\text{MSE} &= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial}{\\partial m}$ and $\\frac{\\partial}{\\partial b}$ are the partial\n",
    "derivatives of the loss function with respect to $m$ and $b$ respectively.\n",
    "\n",
    "Solving the above equations gives us:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m = \\frac{\\sum{y}\\sum{x^2} - \\sum{x}\\sum{xy}}{n\\sum{x^2} - \\sum{x}^2} \\\\\n",
    "b = \\frac{n\\sum{xy} - \\sum{x}\\sum{y}}{n\\sum{x^2} - \\sum{x}^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of data points, $x$ is the input variable, $y$ is the output\n",
    "variable, and $\\sum{x^2}$ is the sum of the squares of the input variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the features and the targets and returns the best values\n",
    "for $m$ and $b$ using the above equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(x: np.ndarray, y: np.ndarray) -> tuple[float, float]:\n",
    "    # Replace with your code\n",
    "    return 0, 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your function by comparing the values of $m$ and $b$ that you get with the expected\n",
    "values. Run the following cell. If your function is correct, the cell will print `Solved`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, b = solve(reservations, pizzas)\n",
    "loss = mean_squared_error(m * reservations + b, pizzas)\n",
    "print(f\"Slope: {m:.2f}, Intercept: {b:.2f}, Loss: {loss:.2f}\")\n",
    "assert np.allclose([m, b], [1.081111957349581, 13.172581873571973]), (m, b)\n",
    "print(\"Solved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values did you get for $m$ and $b$? Are they close to the values that you got\n",
    "using the slider app?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "We can also solve the multiple linear regression problem mathematically. The equation\n",
    "for the best values of $w$ and $b$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w} \\text{MSE} &= 0 \\\\\n",
    "\\frac{\\partial}{\\partial b} \\text{MSE} &= 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial}{\\partial w}$ and $\\frac{\\partial}{\\partial b}$ are the partial\n",
    "derivatives of the loss function with respect to $w$ and $b$ respectively.\n",
    "\n",
    "Solving the above equations gives us:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w = (X^T X)^{-1} X^T y \\\\\n",
    "b = \\bar{y} - w^T \\bar{x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix of features, $y$ is the vector of targets, $\\bar{x}$ is the\n",
    "mean of the features, $\\bar{y}$ is the mean of the targets, and $w$ is the vector of\n",
    "weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the features and the targets and returns the best values\n",
    "for $w$ and $b$ using the above equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_mlr(x: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "    # Replace with your code\n",
    "    # Note 1: The input x is a 2D array with shape (n, k), where n is the number of data\n",
    "    #         points and k is the number of features.\n",
    "    # Note 2: What should the shape of the weights be?\n",
    "    return np.zeros(()), 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test your function. If your function is correct, the cell\n",
    "will print `Solved`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "assert isinstance(X, np.ndarray)\n",
    "assert isinstance(y, np.ndarray)\n",
    "w, b = solve_mlr(X, y)\n",
    "assert np.isclose(mean_squared_error(predict(X, w, b), y), 2859.69634758675)\n",
    "print(\"Solved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "It turns out that solving the linear regression problem mathematically doesn't work\n",
    "well in practice. Our datasets had sizes of 30, and 442. In practice, we will have\n",
    "datasets with millions of data points. Solving the linear regression problem\n",
    "mathematically is computationally expensive. Finding the inverse of a matrix is\n",
    "expensive, and not numerically stable. We can use a technique called **Gradient Descent**\n",
    "to find the best values for the weights and bias."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent in 1D\n",
    "\n",
    "Let's start with the simple linear regression problem. We can visualize the loss\n",
    "function as a curve. The best values for $m$ and $b$ are the values that minimize the\n",
    "loss function. We can use gradient descent to find the values that minimize the loss\n",
    "function.\n",
    "\n",
    "#### The algorithm\n",
    "\n",
    "The algorithm for gradient descent is:\n",
    "\n",
    "1. Initialize the weights and bias randomly.\n",
    "2. Calculate the predicted targets, $\\hat{y}$, using the current weights and bias.\n",
    "3. Calculate the loss, $L$, using the predicted targets and the actual targets.\n",
    "4. Calculate the gradient of the loss with respect to the weights and bias.\n",
    "5. Update the weights and bias using the gradients.\n",
    "6. Repeat steps 2-5 until the loss is small enough.\n",
    "\n",
    "#### Calculating the gradients\n",
    "\n",
    "In practice, we don't calculate the gradients directly. We will\n",
    "\n",
    "a. Use a library that implements the Linear Regression model for us, or\n",
    "b. Use a library that automatically calculates the gradients for us.\n",
    "\n",
    "We will use the second option. We will use the `torch` library to automatically\n",
    "calculate the gradients for us.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `PyTorch` primer\n",
    "\n",
    "A full overview of PyTorch is outside the scope of this tutorial. You can read the\n",
    "`PyTorch` documentation [here](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "PyTorch is a library for automatic differentiation. It can calculate the gradients of\n",
    "functions etc. It has an API that is slightly different from numpy. It can also run on\n",
    "GPUs and TPUs.\n",
    "\n",
    "We will use a `torch.Tensor` in place of a `numpy.ndarray`. This in combination with\n",
    "`torch.func` will allow us to use the `torch` library to automatically calculate the\n",
    "gradients. We will redefine the above functions using `torch.Tensor`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the `predict` function using `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def predict(x: torch.Tensor, weights: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    # Replace with your code\n",
    "    # Note 1: The shape of x is (n, k) where n is the number of samples and k is\n",
    "    #         the number of features\n",
    "    # Note 2: Check that the number of weights is equal to the number of features\n",
    "    return torch.zeros(())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the `mean_squared_error` function using `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(\n",
    "    weights: torch.Tensor, bias: torch.Tensor, x: torch.Tensor, y_true: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    # Replace with your code\n",
    "    # Note 1: We're passing in the parameters as a tuple, so you'll need to unpack them\n",
    "    # Note 2: The shape of x is (n, k) where n is the number of samples and k is\n",
    "    #        the number of features\n",
    "    return torch.as_tensor(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the loss function defined, we can use the `torch.func.grad` function to\n",
    "calculate the gradients. The `grad` function takes in a function and returns\n",
    "a function that calculates the gradients of the input function. We will use the\n",
    "`grad` function to calculate the gradients of the `mean_squared_error` function.\n",
    "\n",
    "The `torch.grad_and_value` function is similar to the `torch.grad` function. It takes\n",
    "in a function and returns a function that calculates the gradients and the value of the\n",
    "input function. We will use the `grad_and_value` function to calculate the gradients\n",
    "and the value of the `mean_squared_error` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.func as ft\n",
    "\n",
    "grad_fn = ft.grad_and_value(mean_squared_error, argnums=(0, 1))  # type: ignore\n",
    "grad_fn: t.Callable[\n",
    "    [torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    tuple[torch.Tensor, torch.Tensor],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(grad_weights, grad_bias), loss = grad_fn(\n",
    "    torch.randn(10), torch.randn(()), torch.randn(10, 10), torch.randn(10)\n",
    ")\n",
    "assert grad_weights.shape == (10,)\n",
    "assert grad_bias.shape == ()\n",
    "assert loss.shape == ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the weights and bias\n",
    "\n",
    "Let us now focus on step 5 of the algorithm. We will use the gradients to update the\n",
    "weights and bias. We will use the following equation to update the weights and bias:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w &= w - \\alpha \\frac{\\partial L}{\\partial w} \\\\\n",
    "b &= b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1. Why do we subtract the gradients from the weights and bias instead of adding them?\n",
    "2. What is the role of the learning rate in gradient descent?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes in the gradients of the loss with respect to the weights\n",
    "and bias and updates the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameter(\n",
    "    param: torch.Tensor, grad: torch.Tensor, lr: float\n",
    ") -> torch.Tensor:\n",
    "    # Replace with your code\n",
    "    return param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training loop\n",
    "\n",
    "We are now ready to implement the training loop. The training loop will repeat the\n",
    "following steps until the loss is small enough:\n",
    "\n",
    "1. Calculate the predicted targets, $\\hat{y}$, using the current weights and bias.\n",
    "2. Calculate the loss, $L$, using the predicted targets and the actual targets.\n",
    "3. Calculate the gradients of the loss with respect to the weights and bias.\n",
    "4. Update the weights and bias using the gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that implements the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    params: tuple[torch.Tensor, torch.Tensor],\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    lr: float,\n",
    ") -> tuple[float, tuple[torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"Performs a single step of gradient descent.\n",
    "\n",
    "    Args:\n",
    "        params: The parameters of the model - a tuple of (weights, bias)\n",
    "        x: The input data\n",
    "        y: The true values\n",
    "        lr: The learning rate\n",
    "\n",
    "    Returns:\n",
    "        A tuple of the loss and the updated parameters\n",
    "    \"\"\"\n",
    "    # Replace with your code\n",
    "    # Note 1: We're passing in the parameters as a tuple, so you'll need to unpack them\n",
    "    # Note 2: The shape of x is (n, k) where n is the number of samples and k is\n",
    "    #         the number of features\n",
    "    # Note 3: You'll need to use the update_parameter function and the grad_fn defined above\n",
    "    # Note 4: This method only implements a single step of gradient descent.\n",
    "    return 0, params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces of the puzzle, we can train a model to predict the\n",
    "diabetes progression from the 10 baseline variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def solve_gd(\n",
    "    features: np.ndarray, targets: np.ndarray, num_epochs=1000, learning_rate=0.001\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Solves the linear regression problem using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        features: A 2D array of shape (num_examples, num_features).\n",
    "        targets: A 1D array of shape (num_examples,).\n",
    "        num_epochs: The number of epochs to train for.\n",
    "        learning_rate: The learning rate.\n",
    "\n",
    "    Returns:\n",
    "        The weights and bias.\n",
    "    \"\"\"\n",
    "    # Setup experiment tracking with WandB.\n",
    "    wandb.init(project=\"linear-regression\")\n",
    "    # Initialize the weights and bias.\n",
    "    # Note: We're using a normal distribution here, but you could use any initialization\n",
    "    #       method you like.\n",
    "    # Note: The seed is used to ensure that the results are reproducible. You can\n",
    "    #       change the value of the seed to see how it affects the results.\n",
    "    seed = 0\n",
    "    torch.manual_seed(seed)\n",
    "    weights = torch.randn(features.shape[1] if features.ndim == 2 else 1)\n",
    "    bias = torch.randn(())\n",
    "    features_ = torch.from_numpy(features).float()\n",
    "    targets_ = torch.from_numpy(targets).float()\n",
    "\n",
    "    # Train the model.\n",
    "    # Setup a progress bar to display the training progress.\n",
    "    pbar = tqdm(total=num_epochs)\n",
    "    for _ in range(num_epochs):\n",
    "        loss, (weights, bias) = train_step(\n",
    "            (weights, bias), features_, targets_, lr=learning_rate\n",
    "        )\n",
    "        # Log the loss and parameters to WandB.\n",
    "        if weights.ndim == 1 and len(weights) > 1:\n",
    "            logged_weights = wandb.Histogram(weights.detach().cpu().numpy())\n",
    "        else:\n",
    "            logged_weights = weights.detach().cpu().item()\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"training/loss\": loss,\n",
    "                \"parameters/weights\": logged_weights,\n",
    "                \"parameters/bias\": bias,\n",
    "            }\n",
    "        )\n",
    "        # Update the progress bar.\n",
    "        pbar.set_description(f\"Loss: {loss:.2f}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Clean up the progress bar.\n",
    "    pbar.close()\n",
    "    # Finish the experiment so that next run is not logged to the same experiment data.\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return the best\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = solve_gd(reservations, pizzas, num_epochs=8000, learning_rate=0.0005)\n",
    "loss = mean_squared_error(\n",
    "    w, b, torch.from_numpy(reservations).float(), torch.from_numpy(pizzas).float()\n",
    ").item()\n",
    "assert 22 <= loss <= 24\n",
    "print(\"Solved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "assert isinstance(X, np.ndarray)\n",
    "assert isinstance(y, np.ndarray)\n",
    "w, b = solve_gd(X, y, num_epochs=3000)\n",
    "loss = mean_squared_error(\n",
    "    w,\n",
    "    b,\n",
    "    torch.as_tensor(X, dtype=torch.float32),\n",
    "    torch.as_tensor(y, dtype=torch.float32),\n",
    ").item()\n",
    "assert w.shape == (10,)\n",
    "print(\"Solved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a better understanding of the effect of the learning rate on the training.\n",
    "The next cell creates an app that allows you to vary the learning rate and see how\n",
    "the training changes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
